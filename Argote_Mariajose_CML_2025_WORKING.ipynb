{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b07875e",
   "metadata": {},
   "source": [
    "# Final Project \n",
    "## Complete Machine Learning Pipeline for MIMIC Classification\n",
    "\n",
    "### Mariajose Argote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebac039",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'asttokens'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "EDA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cbb410",
   "metadata": {},
   "source": [
    "### Objective\n",
    "Build a classification pipeline to predict `HOSPITAL_EXPIRE_FLAG` (patient mortality during ICU stay) using the MIMIC-III dataset subset with 20,885 ICU patient observations.\n",
    "\n",
    "### Success Criteria\n",
    "- Working, reproducible Jupyter notebook with complete ML pipeline\n",
    "- Predictions submitted as probabilities (.predict_proba) in CSV format\n",
    "- Top-tier prediction ranking (within reasonable distance of best performer)\n",
    "- Ability to defend all modeling decisions during in-person presentation\n",
    "- Grade target: 9/10 from presentation + up to 1/10 from prediction ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import jinja2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, TargetEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23935cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random seed for reproducibility\n",
    "# np.random.seed() has a range of [0, 2**32 - 1] for the seed value\n",
    "print(np.random.randint(0, 2**30))\n",
    "\n",
    "SEED = 244459055\n",
    "\n",
    "np.random.seed(SEED)\n",
    "#540921260\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1617b8ca",
   "metadata": {},
   "source": [
    "### 1.1 Import MIMIC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ef4b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "train = pd.read_csv('MIMIC III dataset HEF/mimic_train_HEF.csv')\n",
    "test= pd.read_csv('MIMIC III dataset HEF/mimic_test_HEF.csv')\n",
    "\n",
    "\n",
    "# Display first few rows of the dataset\n",
    "\n",
    "print(train.info())\n",
    "\n",
    "train.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0936be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbfd61e",
   "metadata": {},
   "source": [
    "### 1.2 Check data types, missing values, distributions\n",
    "\n",
    "Visual Inspection\n",
    "* 10% of heart rate and blood pressure columns are missing / blank\n",
    "* 11% of SYS and DiasBP columns are missing /blank\n",
    "* 10% missing respiration rate columns\n",
    "* 12% missing temperature information\n",
    "* 11% missing SPO2 columns information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90eb176",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.info())\n",
    "test.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24243bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check train/test column consistency\n",
    "target_col = \"HOSPITAL_EXPIRE_FLAG\"\n",
    "leaky_cols = ['LOS', 'DOD', 'DISCHTIME','DEATHTIME'] #must drop for modeling\n",
    "id_cols      = [\"subject_id\", \"hadm_id\", \"icustay_id\"]\n",
    "\n",
    "# Groups \n",
    "vital_cols = [  # 24 cols: min/max/mean of each\n",
    "    \n",
    "    \"HeartRate_Min\", \"HeartRate_Max\", \"HeartRate_Mean\",\n",
    "    \"SysBP_Min\", \"SysBP_Max\", \"SysBP_Mean\",\n",
    "    \"DiasBP_Min\", \"DiasBP_Max\", \"DiasBP_Mean\",\n",
    "    \"MeanBP_Min\", \"MeanBP_Max\", \"MeanBP_Mean\",\n",
    "    \"RespRate_Min\", \"RespRate_Max\", \"RespRate_Mean\",\n",
    "    \"TempC_Min\", \"TempC_Max\", \"TempC_Mean\",\n",
    "    \"SpO2_Min\", \"SpO2_Max\", \"SpO2_Mean\",\n",
    "    \"Glucose_Mean\", \"Glucose_Max\", \"Glucose_Min\"\n",
    "]\n",
    "\n",
    "temporal_cols = [\"DOB\", \"ADMITTIME\", \"DISCHTIME\", \"Diff\"]\n",
    "\n",
    "demographic_cols = [\"GENDER\", \"ADMISSION_TYPE\", \"INSURANCE\",\n",
    "             \"RELIGION\", \"MARITAL_STATUS\", \"ETHNICITY\"]\n",
    "\n",
    "clinical_cols = [\"DIAGNOSIS\", \"ICD9_diagnosis\", \"FIRST_CAREUNIT\"]\n",
    "\n",
    "\n",
    "# Columns to use as features in train\n",
    "exclude_cols = set([target_col]) | set(leaky_cols) | set(id_cols)\n",
    "feature_cols = [c for c in train.columns if c not in exclude_cols]\n",
    "\n",
    "# Working copy and drop giveaways up front\n",
    "train = train.copy()\n",
    "train_model = train.drop(columns=leaky_cols, errors=\"ignore\")\n",
    "\n",
    "test_model = test.drop(columns=leaky_cols, errors=\"ignore\")\n",
    "\n",
    "print(train_model.shape, test_model.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785bc417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Check that test has all feature columns\n",
    "missing_in_test = set(feature_cols) - set(test.columns)\n",
    "extra_in_test   = set(test.columns) - set(feature_cols)\n",
    "print(\"Missing in test:\", missing_in_test)\n",
    "print(\"Extra in test  :\", extra_in_test)\n",
    "\n",
    "# 2) Reorder test columns to match train's feature order\n",
    "test = test[feature_cols]\n",
    "\n",
    "\n",
    "#checking for column type consistency\n",
    "shared = set(train_model.columns) & set(test.columns)\n",
    "dtype_diff = {\n",
    "    col: (train_model[col].dtype, test[col].dtype)\n",
    "    for col in shared\n",
    "    if train_model[col].dtype != test[col].dtype\n",
    "}\n",
    "print(\"Columns with different dtypes:\", dtype_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1965186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check missing values - columns\n",
    "\n",
    "def missing_summary(train):\n",
    "    n = len(train)\n",
    "    miss_cnt = train.isna().sum()\n",
    "    miss_pct = miss_cnt / n\n",
    "    return (\n",
    "        pd.DataFrame({\n",
    "            \"n_missing\": miss_cnt,\n",
    "            \"pct_missing\": miss_pct\n",
    "        })\n",
    "        .sort_values(\"pct_missing\", ascending=False)\n",
    "    )\n",
    "\n",
    "col_missing = missing_summary(train.drop(columns=id_cols, errors=\"ignore\")\n",
    "                              .drop(columns=leaky_cols, errors=\"ignore\"))\n",
    "print(col_missing.head(15))\n",
    "\n",
    "#Check missing values - rows\n",
    "row_missing_count = train_model.isna().sum(axis=1)\n",
    "row_missing_frac  = row_missing_count / train_model.shape[1]\n",
    "\n",
    "print(\"Rows with ANY missing:\", (row_missing_count > 0).mean())\n",
    "print(\"Rows with > 20% missing:\", (row_missing_frac > 0.2).mean())\n",
    "print(\"Rows with > 50% missing:\", (row_missing_frac > 0.5).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949c4bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if vital signs are missing together in rows\n",
    "\n",
    "vital_cols_present = [c for c in vital_cols if c in train_model.columns] \n",
    "\n",
    "missing_vitals = train_model[vital_cols_present].isna()\n",
    "\n",
    "# Fraction of rows with at least one missing vital\n",
    "rows_any_vital_missing  = missing_vitals.any(axis=1)\n",
    "# Fraction of rows with all vitals missing\n",
    "rows_all_vitals_missing = missing_vitals.all(axis=1)\n",
    "\n",
    "print(\"Frac rows with ANY vital missing   :\", rows_any_vital_missing.mean().round(3))\n",
    "print(\"Frac rows with ALL vitals missing  :\", rows_all_vitals_missing.mean().round(3))\n",
    "\n",
    "# See patterns of missingness across vitals\n",
    "pattern_counts = missing_vitals.value_counts()\n",
    "print(pattern_counts.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a3c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring those rows where vitals are missing\n",
    "\n",
    "# Count missing + present vitals per row\n",
    "THRESH_MISSING_VITALS = 21\n",
    "\n",
    "def vitals_missing_stats(df, vital_cols):\n",
    "    n_vitals_missing  = df[vital_cols].isna().sum(axis=1)\n",
    "    print(f\"Rows in with 21 vitals missing vitals:\", \n",
    "          (n_vitals_missing == THRESH_MISSING_VITALS).sum())\n",
    "\n",
    "\n",
    "vitals_missing_stats(train_model, vital_cols) \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "vital_cols_wo_glucose = [col for col in vital_cols if \"Glucose\" not in col]\n",
    "train_model_clean = train_model.dropna(subset=vital_cols_wo_glucose, how=\"all\")\n",
    "\n",
    "test_model_clean = test_model.dropna(subset=vital_cols_wo_glucose, how=\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb25523a",
   "metadata": {},
   "source": [
    "Because vital signs are central to our mortality prediction, I removed ICU stays where ≥ 87.5% of vital-sign summary features were missing (2,183 out of 20,885 stays). For the remaining patients, the plan is to impute the missing vitals using median/mean values within the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42512657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking different dataframe sizes\n",
    "\n",
    "print(\"train shape:\", train.shape)\n",
    "print(\"train_model shape:\", train_model.shape)\n",
    "print(\"train_model_clean shape:\", train_model_clean.shape)\n",
    "print(\"test_model shape:\", test_model.shape)\n",
    "print(\"test_model_clean shape:\", test_model_clean.shape)\n",
    "\n",
    "train_model_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fa68c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking missing data by columns after clean drop\n",
    "\n",
    "col_missing = missing_summary(train_model_clean.drop(columns=id_cols, errors=\"ignore\"))\n",
    "col_missing.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3859f7",
   "metadata": {},
   "source": [
    "### 1.3 Identify and visualize class imbalance in target\n",
    "\n",
    "88.8% of ICU stays survived in the cleaned training set, this indicates strong class imbalance in the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a4399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_clean[target_col].value_counts(normalize=True)\n",
    "\n",
    "#88.8% of ICU stays survived in the cleaned training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ea450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting target distribution after cleaning\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "sns.countplot(x=target_col, data=train_model_clean)\n",
    "plt.title(\"Target Distribution: HOSPITAL_EXPIRE_FLAG\")\n",
    "plt.xticks([0, 1], [\"Survived (0)\", \"Died (1)\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1649a1",
   "metadata": {},
   "source": [
    "### 1.4 Examine correlations between vital signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b1026",
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_present = [c for c in vital_cols if c in train_model_clean.columns]\n",
    "\n",
    "vital_means = [c for c in vitals_present if c.endswith(\"_Mean\")]\n",
    "\n",
    "train_model_clean[vital_means].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bb7000",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EDA:\n",
    "    num_features = train_model_clean[feature_cols].select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    ncols = 3\n",
    "    nrows = 5                     # fixed grid: 5 rows x 3 cols = 15 plots per figure\n",
    "    plots_per_fig = nrows * ncols\n",
    "\n",
    "    # 2. Loop over features in chunks of 15\n",
    "    for start in range(0, len(num_features), plots_per_fig):\n",
    "        subset = num_features[start:start + plots_per_fig]\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=nrows, ncols=ncols,\n",
    "                                 figsize=(15, 5 * nrows))\n",
    "\n",
    "        # Make axes always a flat 1D array for simple indexing\n",
    "        axes = np.array(axes).ravel()\n",
    "\n",
    "        # 3. Draw each boxplot\n",
    "        for i, feat in enumerate(subset):\n",
    "            ax = axes[i]\n",
    "\n",
    "            # Optional: sample rows if dataset is big to speed up plotting\n",
    "            data_to_plot = train[feat]\n",
    "            if len(train_model_clean) > 5000:\n",
    "                data_to_plot = data_to_plot.sample(5000, random_state=0)\n",
    "\n",
    "            sns.boxplot(y=data_to_plot, ax=ax)\n",
    "            ax.set_title(feat)\n",
    "\n",
    "        # 4. Turn off any leftover empty axes\n",
    "        for j in range(len(subset), len(axes)):\n",
    "            axes[j].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af5a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we analyze the correlation matrix of the numerical columns\n",
    "\n",
    "if EDA:\n",
    "    corr = train_model_clean[num_features].corr()\n",
    "    display(corr.style.background_gradient(cmap='coolwarm', axis=None).format(\"{:.4f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b278d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#focusing on vital means correlation heatmap\n",
    "\n",
    "corr = train_model_clean[vital_means].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=False, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Between Mean Vital Signs\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f519e75",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Heart rate and respiration rate are moderately positively correlated\n",
    "* Systolic, Diastolic BP and MeanBP are highly correlated, as expected\n",
    "* SpO2 and Respiratory Rate are negatively correlated, as expected\n",
    "* Heart rate and Respiration Rate and TempC are positively correlated\n",
    "* Glucose is negatively correlated with Temperature and SpO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fb3f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean vitals by survival vs death\n",
    "grouped_vitals = (\n",
    "    train_model_clean\n",
    "    .groupby(target_col)[vital_means]\n",
    "    .mean()\n",
    "    .T\n",
    ")\n",
    "\n",
    "grouped_vitals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21380a18",
   "metadata": {},
   "source": [
    "### 1.5 Explore categorical variable distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2548d85e",
   "metadata": {},
   "source": [
    "### 2.1 Feature Engineering age at admission, time-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9744e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for age and time features (used later for test data)\n",
    "\n",
    "def add_age_and_time_features(df):\n",
    "    df = df.copy()\n",
    "    # Convert to datetime\n",
    "    df['DOB'] = pd.to_datetime(df['DOB'], errors='coerce')\n",
    "    df['ADMITTIME'] = pd.to_datetime(df['ADMITTIME'], errors='coerce')\n",
    "\n",
    "    df['AGE'] = np.nan\n",
    "    validmask = df['DOB'].notna() & df['ADMITTIME'].notna()\n",
    "    if validmask.any():\n",
    "        dob = df.loc[validmask, 'DOB']\n",
    "        admit = df.loc[validmask, 'ADMITTIME']\n",
    "\n",
    "        years = admit.dt.year - dob.dt.year\n",
    "        hadbirthday = (\n",
    "            (admit.dt.month > dob.dt.month) |\n",
    "            ((admit.dt.month == dob.dt.month) & (admit.dt.day >= dob.dt.day))\n",
    "        )\n",
    "        ageyears = years - (~hadbirthday).astype(int)\n",
    "        df.loc[validmask, 'AGE'] = ageyears\n",
    "\n",
    "    # clamp to [0,120]\n",
    "    maskinvalid = (df['AGE'] < 0) | (df['AGE'] > 120)\n",
    "    df.loc[maskinvalid, 'AGE'] = np.nan\n",
    "\n",
    "    print(df[\"AGE\"].describe())\n",
    "    missing_mask = df[\"AGE\"].isna()\n",
    "    n_missing = missing_mask.sum()\n",
    "    n_total = len(df)\n",
    "    print(f\"Missing AGE: {n_missing} / {n_total} ({n_missing/n_total:.3f})\")\n",
    "\n",
    "    # mortality among rows with missing AGE\n",
    "    mortality_missing = df.loc[missing_mask, target_col].mean()\n",
    "    # mortality among rows with known AGE\n",
    "    mortality_known   = df.loc[~missing_mask, target_col].mean()\n",
    "\n",
    "    print(f\"Mortality (AGE missing): {mortality_missing:.3f}\")\n",
    "    print(f\"Mortality (AGE known)  : {mortality_known:.3f}\")\n",
    "\n",
    "    # simple time features\n",
    "    df['admit_hour'] = df['ADMITTIME'].dt.hour\n",
    "    df['admit_weekday'] = df['ADMITTIME'].dt.dayofweek\n",
    "    df['is_weekend'] = df['admit_weekday'].isin([5, 6]).astype(int)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply function to clean train data and plot age distribution\n",
    "\n",
    "train_base = add_age_and_time_features(train_model_clean)\n",
    "\n",
    "print(train_base.shape)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(train_base[\"AGE\"], bins=40, kde=True)\n",
    "plt.title(\"Age Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(x=target_col, y=\"AGE\", data=train_base)\n",
    "plt.title(\"Age by Mortality Outcome\")\n",
    "plt.xticks([0, 1], [\"Survived\", \"Died\"])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420700cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in demographic_cols:\n",
    "    if col in train_base.columns:\n",
    "        print(f\"\\n=== {col} ===\")\n",
    "        print(train_base[col].value_counts(normalize=True).head(10))\n",
    "\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.countplot(\n",
    "            y=col,\n",
    "            data=train_base,\n",
    "            order=train_base[col].value_counts().index\n",
    "        )\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a98c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking specific key categories against target\n",
    "cat_for_target = [\"GENDER\",\"ETHNICITY\",\"INSURANCE\", \"FIRST_CAREUNIT\"]\n",
    "\n",
    "for col in cat_for_target:\n",
    "    if col in train_base.columns:\n",
    "        # Mortality rate by category\n",
    "        rate = (\n",
    "            train_base\n",
    "            .groupby(col)[target_col]\n",
    "            .mean()\n",
    "            .sort_values(ascending=False)\n",
    "        )\n",
    "        print(f\"\\nMortality rate by {col}:\")\n",
    "        print(rate)\n",
    "\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.barplot(x=rate.values, y=rate.index)\n",
    "        plt.title(f\"Mortality Rate by {col}\")\n",
    "        plt.xlabel(\"Mean HOSPITAL_EXPIRE_FLAG\")\n",
    "        plt.ylabel(col)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca729fe6",
   "metadata": {},
   "source": [
    "### Phase 2.2 Diagnoses Extra Data EDA & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed7262",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load diagnoses file\n",
    "MIMIC_diagnoses = pd.read_csv(\"MIMIC III dataset HEF/extra_data/MIMIC_diagnoses.csv\")\n",
    "\n",
    "MIMIC_diagnoses.head()\n",
    "MIMIC_diagnoses.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e4dbd",
   "metadata": {},
   "source": [
    "#### 2.2.1 Creating diagnoses feature fitting for train data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e71c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#restrict diagnoses to only those in train_model_clean\n",
    "\n",
    "def fit_diagnosis_features(train_df, diagnoses_df):\n",
    "    df = train_df.copy()\n",
    "    \n",
    "    # Restrict diagnoses to hadm_id present in train\n",
    "    diagnoses_train = diagnoses_df[diagnoses_df['HADM_ID'].isin(df['hadm_id'])].copy()\n",
    "\n",
    "    # Merge labels\n",
    "    diagnoses_train = diagnoses_train.merge(\n",
    "        df[['hadm_id', 'HOSPITAL_EXPIRE_FLAG']],\n",
    "        left_on='HADM_ID', \n",
    "        right_on='hadm_id', \n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Count diagnoses per admission\n",
    "    diagnoses_counts = diagnoses_train.groupby('hadm_id').size().reset_index(name='diagnosis_count')\n",
    "\n",
    "    # Mortality per ICD9\n",
    "    severity = (\n",
    "        diagnoses_train\n",
    "        .groupby('ICD9_CODE')['HOSPITAL_EXPIRE_FLAG']\n",
    "        .agg(['sum', 'count'])\n",
    "        .rename(columns={'sum': 'n_deaths', 'count': 'n_patients'})\n",
    "    )\n",
    "    overall_mort = df['HOSPITAL_EXPIRE_FLAG'].mean()\n",
    "    alpha = 10\n",
    "\n",
    "    severity['raw_mortality'] = severity['n_deaths'] / severity['n_patients']\n",
    "    severity['smoothed_severity'] = (\n",
    "        (severity['n_patients'] * severity['raw_mortality'] + alpha * overall_mort)\n",
    "        / (severity['n_patients'] + alpha)\n",
    "    )\n",
    "\n",
    "    # Save mapping for later use\n",
    "    severity_map = severity['smoothed_severity'].to_dict()\n",
    "\n",
    "    # Attach severity to all diagnoses\n",
    "    diagnoses_train['severity'] = diagnoses_train['ICD9_CODE'].map(severity_map)\n",
    "    diagnoses_train['severity'] = diagnoses_train['severity'].fillna(overall_mort)\n",
    "\n",
    "    # Aggregate per hadm_id\n",
    "    sev_agg = diagnoses_train.groupby('hadm_id')['severity'].agg(\n",
    "        avg_diagnosis_severity='mean',\n",
    "        max_diagnosis_severity='max'\n",
    "    ).reset_index()  # ← RESET INDEX HERE\n",
    "\n",
    "    # Primary diagnosis severity\n",
    "    primary = diagnoses_train[diagnoses_train['SEQ_NUM'] == 1].copy()\n",
    "    primary = (primary\n",
    "               .groupby('hadm_id')['severity']\n",
    "               .first()  # In case multiple primaries (shouldn't happen but safeguard)\n",
    "               .reset_index()\n",
    "               .rename(columns={'severity': 'primary_diagnosis_severity'}))\n",
    "\n",
    "    # merge features back to df\n",
    "    df = df.merge(diagnoses_counts, on='hadm_id', how='left')\n",
    "    df = df.merge(sev_agg, on='hadm_id', how='left')\n",
    "    df = df.merge(primary, on='hadm_id', how='left')\n",
    "\n",
    "    # Fill missing (no diagnoses)\n",
    "    df['diagnosis_count'] = df['diagnosis_count'].fillna(0)\n",
    "    for col in ['avg_diagnosis_severity', 'max_diagnosis_severity', 'primary_diagnosis_severity']:\n",
    "        df[col] = df[col].fillna(overall_mort)\n",
    "\n",
    "    # Pack state\n",
    "    state = {\n",
    "        'severity_map': severity_map,\n",
    "        'overall_mort': overall_mort,\n",
    "        'alpha': alpha,\n",
    "    }\n",
    "    return df, state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa40b0",
   "metadata": {},
   "source": [
    "#### Severity Quick Comments\n",
    "\n",
    "**Severity** = mortality risk score per ICD9 diagnosis code\n",
    "\n",
    "**How it's calculated:**\n",
    "1. For each ICD9 code in the training data, compute the proportion of patients who died\n",
    "2. Apply Bayesian smoothing to balance observed rates with the overall mortality rate\n",
    "3. Rare diagnoses (few patients) shrink toward the average; common diagnoses stay close to observed rates\n",
    "\n",
    "**Why Bayesian smoothing?**\n",
    "- Prevents extreme estimates: A rare code with 1 patient who died would have raw mortality = 100%, which is unreliable. With smoothing (α=10), that becomes: (1 × 1.0 + 10 × 0.112) / 11 ≈ 0.19 (more conservative). Common codes with 100+ patients are barely affected by the prior\n",
    "\n",
    "**α = 10 interpretation:**\n",
    "- Acts like adding 10 \"virtual patients\" with average mortality to every ICD9 code\n",
    "- Balances between observed data (for common codes) and prior knowledge (for rare codes)\n",
    "\n",
    "Each ICD9 code gets a severity score between 0 (low mortality risk) and 1 (high mortality risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to clean train data\n",
    "train_base_w_severity, diag_state = fit_diagnosis_features(train_base, MIMIC_diagnoses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e5ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_base_w_severity.shape, train_model_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6907db3",
   "metadata": {},
   "source": [
    "#### 2.2.2 Function to apply diagnoses severity features to test data later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74500cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply to test data function\n",
    "\n",
    "def apply_diagnosis_features(df, diagnoses_df, state):\n",
    "    df = df.copy()\n",
    "    severity_map = state['severity_map']\n",
    "    overall_mort = state['overall_mort']\n",
    "\n",
    "    diag = diagnoses_df[diagnoses_df['HADM_ID'].isin(df['hadm_id'])].copy()\n",
    "\n",
    "    diag['severity'] = diag['ICD9_CODE'].map(severity_map)\n",
    "    diag['severity'] = diag['severity'].fillna(overall_mort)\n",
    "\n",
    "    diag_counts = diag.groupby('hadm_id').size().rename('diagnosis_count')\n",
    "    sev_agg = diag.groupby('hadm_id')['severity'].agg(\n",
    "        avg_diagnosis_severity='mean',\n",
    "        max_diagnosis_severity='max'\n",
    "    )\n",
    "    primary = diag[diag['SEQ_NUM'] == 1].copy()\n",
    "    primary = primary.set_index('hadm_id')['severity'].rename('primary_diagnosis_severity')\n",
    "\n",
    "    df = df.join(diag_counts, on='hadm_id')\n",
    "    df = df.join(sev_agg, on='hadm_id')\n",
    "    df = df.join(primary, on='hadm_id')\n",
    "\n",
    "    df['diagnosis_count'] = df['diagnosis_count'].fillna(0)\n",
    "    for col in ['avg_diagnosis_severity', 'max_diagnosis_severity', 'primary_diagnosis_severity']:\n",
    "        df[col] = df[col].fillna(overall_mort)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a4b1e7",
   "metadata": {},
   "source": [
    "#### 2.2.3 Distribution of diagnoses per patient at admission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af256e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Diagnoses per Patient\n",
    "\n",
    "# Filter diagnoses to training admissions only\n",
    "train_hadm = set(train_base_w_severity['hadm_id'])\n",
    "diag_train = MIMIC_diagnoses[MIMIC_diagnoses['HADM_ID'].isin(train_hadm)].copy()\n",
    "\n",
    "# Count diagnoses per admission\n",
    "diag_per_patient = diag_train.groupby('HADM_ID').size()\n",
    "\n",
    "print(\"=== Diagnoses per ICU Stay (Training Data) ===\\n\")\n",
    "print(diag_per_patient.describe())\n",
    "print(f\"\\nMedian: {diag_per_patient.median():.0f} diagnoses\")\n",
    "print(f\"Range: {diag_per_patient.min():.0f} to {diag_per_patient.max():.0f} diagnoses\")\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(diag_per_patient, bins=40, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "plt.xlabel('Number of Diagnoses per ICU Stay', fontsize=12)\n",
    "plt.ylabel('Frequency (Number of ICU Stays)', fontsize=12)\n",
    "plt.title('Distribution of Diagnosis Count per ICU Stay', fontsize=14, fontweight='bold')\n",
    "plt.axvline(diag_per_patient.median(), color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Median: {diag_per_patient.median():.0f}')\n",
    "plt.axvline(diag_per_patient.mean(), color='orange', linestyle='--', linewidth=2,\n",
    "            label=f'Mean: {diag_per_patient.mean():.1f}')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e222f3",
   "metadata": {},
   "source": [
    "##### Observation: Diagnosis Volume\n",
    "\n",
    "- **Median:** ~14 diagnoses per ICU stay\n",
    "- **Range:** 1 to 39 diagnoses\n",
    "- **Distribution:** Right-skewed; most patients have 10-20 diagnoses\n",
    "- **Interpretation:** \n",
    "  - Patients with very few diagnoses may represent simpler cases, shorter stays\n",
    "  - Patients with many diagnoses likely have complex, multi-system conditions\n",
    "  - Diagnosis count is likely a good proxy for patient complexity/severity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1848b335",
   "metadata": {},
   "source": [
    "#### 2.2.4 Most Common Primary Diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Common Primary Diagnoses\n",
    "\n",
    "# Extract primary diagnoses (SEQ_NUM == 1)\n",
    "primary_diag = diag_train[diag_train['SEQ_NUM'] == 1].copy()\n",
    "\n",
    "# Top 20 most frequent primary ICD9 codes\n",
    "top_20_primary = primary_diag['ICD9_CODE'].value_counts().head(20)\n",
    "\n",
    "metadata = pd.read_csv(\"MIMIC III dataset HEF/extra_data/MIMIC_metadata_diagnose.csv\")\n",
    "top_20_df = top_20_primary.reset_index()\n",
    "top_20_df.columns = ['ICD9_CODE', 'count']\n",
    "\n",
    "top_20_with_names = top_20_df.merge(\n",
    "    metadata[['ICD9_CODE', 'SHORT_DIAGNOSE']], \n",
    "    on='ICD9_CODE', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"\\n=== Top 20 Primary Diagnoses with Descriptions ===\\n\")\n",
    "print(top_20_with_names.to_string(index=False))\n",
    "\n",
    "# Calculate coverage\n",
    "total_primary = len(primary_diag)\n",
    "top_20_coverage = (top_20_primary.sum() / total_primary) * 100\n",
    "print(f\"\\nTop 20 codes cover {top_20_coverage:.1f}% of all primary diagnoses\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_20_primary.sort_values().plot(kind='barh', color='coral', edgecolor='black')\n",
    "plt.xlabel('Number of Patients', fontsize=12)\n",
    "plt.ylabel('ICD9 Code', fontsize=12)\n",
    "plt.title('Top 20 Most Common Primary Diagnoses', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7e8c65",
   "metadata": {},
   "source": [
    "##### Observation: Primary Diagnoses\n",
    "\n",
    "- **Top 20 codes** represent the most common critical care admission reasons\n",
    "- These 20 codes account for **33%** of all primary diagnoses (check output)\n",
    "- **Considerations for Model**\n",
    "  - Common codes have reliable mortality statistics\n",
    "  - Rare codes (appearing <10 times) will benefit from Bayesian smoothing (further supporting rationale to use Bayesian smoothing)\n",
    "  - Will likely one-hot encode top N (TBD) primary diagnoses as additional features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e35b4a",
   "metadata": {},
   "source": [
    "#### 2.2.5 Diagnosis and Mortality Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4084219",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_counts = train_base_w_severity[['hadm_id', 'HOSPITAL_EXPIRE_FLAG', 'diagnosis_count']].copy()\n",
    "\n",
    "# Bin diagnosis count\n",
    "train_with_counts['diag_bin'] = pd.cut(\n",
    "    train_with_counts['diagnosis_count'],\n",
    "    bins=[0, 5, 10, 15, 20, 25, 40],\n",
    "    labels=['1-5', '6-10', '11-15', '16-20', '21-25', '26+']\n",
    ")\n",
    "\n",
    "# Mortality rate by bin\n",
    "mortality_by_count = (\n",
    "    train_with_counts\n",
    "    .groupby('diag_bin', observed=True)['HOSPITAL_EXPIRE_FLAG']\n",
    "    .agg(['mean', 'count'])\n",
    "    .rename(columns={'mean': 'mortality_rate', 'count': 'n_patients'})\n",
    ")\n",
    "\n",
    "print(\"Mortality Rate by Diagnosis Count\\n\")\n",
    "print(mortality_by_count)\n",
    "print(f\"\\nOverall mortality rate: {train_base['HOSPITAL_EXPIRE_FLAG'].mean():.3f}\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Mortality rate\n",
    "axes[0].bar(mortality_by_count.index.astype(str), \n",
    "            mortality_by_count['mortality_rate'], \n",
    "            color='coral', edgecolor='black', alpha=0.8)\n",
    "axes[0].set_xlabel('Number of Diagnoses', fontsize=12)\n",
    "axes[0].set_ylabel('Mortality Rate', fontsize=12)\n",
    "axes[0].set_title('Mortality Rate by Diagnosis Count', fontsize=13, fontweight='bold')\n",
    "axes[0].axhline(train_base['HOSPITAL_EXPIRE_FLAG'].mean(), \n",
    "                color='blue', linestyle='--', linewidth=2, \n",
    "                label=f\"Overall: {train_base['HOSPITAL_EXPIRE_FLAG'].mean():.3f}\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Sample size\n",
    "axes[1].bar(mortality_by_count.index.astype(str), \n",
    "            mortality_by_count['n_patients'], \n",
    "            color='steelblue', edgecolor='black', alpha=0.8)\n",
    "axes[1].set_xlabel('Number of Diagnoses', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Patients', fontsize=12)\n",
    "axes[1].set_title('Sample Size by Diagnosis Count', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b27c4e",
   "metadata": {},
   "source": [
    "##### comment\n",
    "\n",
    "- **Clear trend:** Mortality rate increases with diagnosis count\n",
    "- **Low count (1-5):** Lowest mortaility rate at 4.1%\n",
    "- **High count (26+):** Highest mortality rate at 18.7%\n",
    "- **Interpretation:**\n",
    "  - More diagnoses → more complex/severe medical conditions\n",
    "  - `diagnosis_count` is a valuable predictor for mortality risk \n",
    "  - Very low counts may indicate coding issues or very short stays, but should stil watch out for few counts, but high severity diagnoses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b5d87",
   "metadata": {},
   "source": [
    "#### 2.2.6 Diagnosis Severity vs Mortaility\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04e787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling from function fit_diagnosis_features() created to pull diagnosis severity features into clean train data \n",
    "severity_cols = ['avg_diagnosis_severity', 'max_diagnosis_severity', \n",
    "                 'primary_diagnosis_severity']\n",
    "\n",
    "# Check if severity features exist\n",
    "if all(col in train_base_w_severity.columns for col in severity_cols):\n",
    "    \n",
    "    train_with_sev = train_base_w_severity[['HOSPITAL_EXPIRE_FLAG'] + severity_cols].copy()\n",
    "    \n",
    "    # Bin by avg severity quartiles\n",
    "    train_with_sev['severity_quartile'] = pd.qcut(\n",
    "        train_with_sev['avg_diagnosis_severity'],\n",
    "        q=4,\n",
    "        labels=['Q1 (Low)', 'Q2', 'Q3', 'Q4 (High)'],\n",
    "        duplicates='drop'\n",
    "    )\n",
    "    \n",
    "    mortality_by_sev = (\n",
    "        train_with_sev\n",
    "        .groupby('severity_quartile', observed=True)['HOSPITAL_EXPIRE_FLAG']\n",
    "        .agg(['mean', 'count'])\n",
    "        .rename(columns={'mean': 'mortality_rate', 'count': 'n_patients'})\n",
    "    )\n",
    "    \n",
    "    print(\"Mortality Rate by Average Diagnosis Severity Quartile\\n\")\n",
    "    print(mortality_by_sev)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Mortality by quartile\n",
    "    axes[0].bar(mortality_by_sev.index.astype(str), \n",
    "                mortality_by_sev['mortality_rate'],\n",
    "                color='darkred', edgecolor='black', alpha=0.8)\n",
    "    axes[0].set_xlabel('Average Diagnosis Severity Quartile', fontsize=12)\n",
    "    axes[0].set_ylabel('Mortality Rate', fontsize=12)\n",
    "    axes[0].set_title('Mortality Rate by Diagnosis Severity', fontsize=13, fontweight='bold')\n",
    "    axes[0].axhline(train_base['HOSPITAL_EXPIRE_FLAG'].mean(), \n",
    "                    color='blue', linestyle='--', linewidth=2,\n",
    "                    label=f\"Overall: {train_base['HOSPITAL_EXPIRE_FLAG'].mean():.3f}\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Sample size\n",
    "    axes[1].bar(mortality_by_sev.index.astype(str), \n",
    "                mortality_by_sev['n_patients'],\n",
    "                color='steelblue', edgecolor='black', alpha=0.8)\n",
    "    axes[1].set_xlabel('Average Diagnosis Severity Quartile', fontsize=12)\n",
    "    axes[1].set_ylabel('Number of Patients', fontsize=12)\n",
    "    axes[1].set_title('Sample Size by Severity Quartile', fontsize=13, fontweight='bold')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional: Show range of severity values\n",
    "    print(\"\\nDiagnosis Severity Feature Ranges\\n\")\n",
    "    print(train_base_w_severity[severity_cols].describe())\n",
    "    \n",
    "else:\n",
    "    print(\"Severity features not yet computed. Run fit_diagnosis_features() first.\")\n",
    "\n",
    "print(train_base_w_severity.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bad85b",
   "metadata": {},
   "source": [
    "### 3 Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4161ed5",
   "metadata": {},
   "source": [
    "#### 3.1 preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648fd818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating pre-processing function to apply to both train and test data\n",
    "\n",
    "def fit_preprocessing_pipeline(df, target_col='HOSPITAL_EXPIRE_FLAG'):\n",
    "    \"\"\"\n",
    "    Fit all preprocessing transformers on training data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Training data with target column\n",
    "    target_col : str\n",
    "        Name of target variable\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df_processed : DataFrame\n",
    "        Fully preprocessed training data\n",
    "    X_train : DataFrame\n",
    "        Features only\n",
    "    y_train : Series\n",
    "        Target only\n",
    "    state : dict\n",
    "        Dictionary containing all fitted transformers\n",
    "    \"\"\"\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import numpy as np\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"=== FITTING PREPROCESSING PIPELINE ===\\n\")\n",
    "    \n",
    "    # =============================\n",
    "    # 1. IDENTIFY FEATURE TYPES\n",
    "    # =============================\n",
    "    \n",
    "    vital_cols = [\n",
    "        'HeartRate_Min', 'HeartRate_Max', 'HeartRate_Mean',\n",
    "        'SysBP_Min', 'SysBP_Max', 'SysBP_Mean',\n",
    "        'DiasBP_Min', 'DiasBP_Max', 'DiasBP_Mean',\n",
    "        'MeanBP_Min', 'MeanBP_Max', 'MeanBP_Mean',\n",
    "        'RespRate_Min', 'RespRate_Max', 'RespRate_Mean',\n",
    "        'TempC_Min', 'TempC_Max', 'TempC_Mean',\n",
    "        'SpO2_Min', 'SpO2_Max', 'SpO2_Mean',\n",
    "        'Glucose_Min', 'Glucose_Max', 'Glucose_Mean'\n",
    "    ]\n",
    "    \n",
    "    cat_cols = ['GENDER', 'ADMISSION_TYPE', 'INSURANCE', 'RELIGION', \n",
    "                'MARITAL_STATUS', 'ETHNICITY', 'FIRST_CAREUNIT']\n",
    "    \n",
    "    # Filter to existing columns\n",
    "    vital_cols = [c for c in vital_cols if c in df.columns]\n",
    "    cat_cols = [c for c in cat_cols if c in df.columns]\n",
    "    \n",
    "    print(f\"Vitals to impute: {len(vital_cols)}\")\n",
    "    print(f\"Categoricals to encode: {len(cat_cols)}\")\n",
    "    \n",
    "    # =============================\n",
    "    # 2. IMPUTE MISSING VALUES\n",
    "    # =============================\n",
    "    \n",
    "    # Vitals: median\n",
    "    imputer_vitals = SimpleImputer(strategy='median')\n",
    "    df[vital_cols] = imputer_vitals.fit_transform(df[vital_cols])\n",
    "    \n",
    "    # Categoricals: mode\n",
    "    imputer_cats = SimpleImputer(strategy='most_frequent')\n",
    "    df[cat_cols] = imputer_cats.fit_transform(df[cat_cols])\n",
    "    \n",
    "    print(f\"✓ Imputed vitals and categoricals\")\n",
    "    \n",
    "    # =============================\n",
    "    # 3. ONE-HOT ENCODE CATEGORICALS\n",
    "    # =============================\n",
    "    \n",
    "    df_encoded = pd.get_dummies(\n",
    "        df, \n",
    "        columns=cat_cols, \n",
    "        drop_first=True,\n",
    "        dtype=int\n",
    "    )\n",
    "    \n",
    "    # Save the columns after encoding for test set alignment\n",
    "    encoded_columns = df_encoded.columns.tolist()\n",
    "    \n",
    "    print(f\"✓ Encoded {len(cat_cols)} categoricals → {df_encoded.shape[1] - df.shape[1]} new features\")\n",
    "    \n",
    "    # =============================\n",
    "    # 4. SEPARATE FEATURES & TARGET\n",
    "    # =============================\n",
    "    \n",
    "    y = df_encoded[target_col]\n",
    "    X = df_encoded.drop(columns=[target_col])\n",
    "    \n",
    "    # =============================\n",
    "    # 5. SCALE NUMERICAL FEATURES\n",
    "    # =============================\n",
    "    \n",
    "    # Identify numerical columns (exclude binary dummies)\n",
    "    num_cols = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    binary_cols = [c for c in num_cols if X[c].nunique() == 2]\n",
    "    num_to_scale = [c for c in num_cols if c not in binary_cols]\n",
    "    \n",
    "    print(f\"✓ Scaling {len(num_to_scale)} numerical features (excluding {len(binary_cols)} binary)\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X[num_to_scale] = scaler.fit_transform(X[num_to_scale])\n",
    "    \n",
    "    # =============================\n",
    "    # 6. FINAL VERIFICATION\n",
    "    # =============================\n",
    "    \n",
    "    assert X.isnull().sum().sum() == 0, \"Missing values remain!\"\n",
    "    assert len(X.select_dtypes(include=['object']).columns) == 0, \"Non-numeric features remain!\"\n",
    "    assert not np.isinf(X.values).any(), \"Infinite values found!\"\n",
    "    \n",
    "    print(f\"\\n✓ PREPROCESSING COMPLETE\")\n",
    "    print(f\"  Final shape: {X.shape}\")\n",
    "    print(f\"  Target distribution: {y.value_counts(normalize=True).to_dict()}\")\n",
    "    \n",
    "    # =============================\n",
    "    # 7. PACK STATE FOR TEST SET\n",
    "    # =============================\n",
    "    \n",
    "    state = {\n",
    "        'imputer_vitals': imputer_vitals,\n",
    "        'imputer_cats': imputer_cats,\n",
    "        'scaler': scaler,\n",
    "        'vital_cols': vital_cols,\n",
    "        'cat_cols': cat_cols,\n",
    "        'num_to_scale': num_to_scale,\n",
    "        'encoded_columns': encoded_columns,\n",
    "        'feature_columns': X.columns.tolist()  # Final feature order\n",
    "    }\n",
    "    \n",
    "    return df_encoded, X, y, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d2333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Preprocessing to Test Set \n",
    "\n",
    "def apply_preprocessing_pipeline(df, state):\n",
    "    \"\"\"\n",
    "    Apply fitted preprocessing transformers to new data (test set).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Test data (without target column)\n",
    "    state : dict\n",
    "        Dictionary of fitted transformers from fit_preprocessing_pipeline()\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_test : DataFrame\n",
    "        Preprocessed features, aligned with training features\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"=== APPLYING PREPROCESSING PIPELINE ===\\n\")\n",
    "    \n",
    "    # =============================\n",
    "    # 1. IMPUTE MISSING VALUES\n",
    "    # =============================\n",
    "    \n",
    "    df[state['vital_cols']] = state['imputer_vitals'].transform(df[state['vital_cols']])\n",
    "    df[state['cat_cols']] = state['imputer_cats'].transform(df[state['cat_cols']])\n",
    "    \n",
    "    print(f\"✓ Imputed vitals and categoricals\")\n",
    "    \n",
    "    # =============================\n",
    "    # 2. ONE-HOT ENCODE CATEGORICALS\n",
    "    # =============================\n",
    "    \n",
    "    df_encoded = pd.get_dummies(\n",
    "        df,\n",
    "        columns=state['cat_cols'],\n",
    "        drop_first=True,\n",
    "        dtype=int\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Encoded categoricals\")\n",
    "    \n",
    "    # =============================\n",
    "    # 3. ALIGN COLUMNS WITH TRAINING\n",
    "    # =============================\n",
    "    \n",
    "    # Add missing columns (with 0s)\n",
    "    missing_cols = set(state['feature_columns']) - set(df_encoded.columns)\n",
    "    for col in missing_cols:\n",
    "        df_encoded[col] = 0\n",
    "    \n",
    "    # Remove extra columns not in training\n",
    "    extra_cols = set(df_encoded.columns) - set(state['feature_columns'])\n",
    "    df_encoded = df_encoded.drop(columns=list(extra_cols))\n",
    "    \n",
    "    # Reorder to match training\n",
    "    X = df_encoded[state['feature_columns']]\n",
    "    \n",
    "    print(f\"✓ Aligned columns with training data\")\n",
    "    print(f\"  Added {len(missing_cols)} missing columns\")\n",
    "    print(f\"  Removed {len(extra_cols)} extra columns\")\n",
    "    \n",
    "    # =============================\n",
    "    # 4. SCALE NUMERICAL FEATURES\n",
    "    # =============================\n",
    "    \n",
    "    X[state['num_to_scale']] = state['scaler'].transform(X[state['num_to_scale']])\n",
    "    \n",
    "    print(f\"✓ Scaled {len(state['num_to_scale'])} numerical features\")\n",
    "    \n",
    "    # =============================\n",
    "    # 5. FINAL VERIFICATION\n",
    "    # =============================\n",
    "    \n",
    "    assert X.shape[1] == len(state['feature_columns']), \"Column mismatch!\"\n",
    "    assert X.isnull().sum().sum() == 0, \"Missing values remain!\"\n",
    "    assert not np.isinf(X.values).any(), \"Infinite values found!\"\n",
    "    \n",
    "    print(f\"\\n✓ TEST PREPROCESSING COMPLETE\")\n",
    "    print(f\"  Final shape: {X.shape}\")\n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed31b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL: Preprocess Training Data ===\n",
    "\n",
    "# Fit pipeline on training data\n",
    "train_processed, X_train, y_train, preprocess_state = fit_preprocessing_pipeline(\n",
    "    train_base_w_severity,target_col='HOSPITAL_EXPIRE_FLAG'\n",
    ")\n",
    "\n",
    "# Save state for test set\n",
    "import joblib\n",
    "joblib.dump(preprocess_state, 'preprocessing_state.pkl')\n",
    "print(\"\\n✓ Saved preprocessing state for test set\")\n",
    "\n",
    "# Quick check\n",
    "print(f\"\\nReady for modeling:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262be17d",
   "metadata": {},
   "source": [
    "### Test file Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Apply diagnosis features firsts\n",
    "test_with_diag = apply_diagnosis_features(test, MIMIC_diagnoses, diag_state)\n",
    "\n",
    "# Apply preprocessing pipeline\n",
    "X_test = apply_preprocessing_pipeline(test_with_diag, preprocess_state)\n",
    "\n",
    "print(f\"\\nTest data ready for predictions:\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "\n",
    "# Verify alignment\n",
    "assert X_test.shape[1] == X_train.shape[1], \"Feature count mismatch!\"\n",
    "print(\"✓ Test features perfectly aligned with training features\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
