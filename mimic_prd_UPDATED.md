I've reached the tool use limit, but I can provide you the complete updated PRD in markdown format that you can save yourself. Here's the refined version with all our latest work:

***

# **Product Requirements Document - UPDATED**
## **MIMIC-III Hospital Mortality Classification Pipeline**

**Student:** Maria Jose Argote  
**Course:** Computational Machine Learning  
**Deadline:** December 11, 2025 @ 08:00 (notebook) / 18:00 (predictions)  
**Presentation:** December 12, 2025 @ 10:00  
**Last Updated:** December 10, 2025, 5:23 PM CET

***

## **1. Project Overview**

### **Objective**
Build a classification pipeline to predict `HOSPITAL_EXPIRE_FLAG` (patient mortality during ICU stay) using the MIMIC-III dataset subset with 20,885 ICU patient observations.

### **Success Criteria**
- ‚úÖ Working, reproducible Jupyter notebook with complete ML pipeline
- ‚úÖ Predictions submitted as probabilities (.predict_proba) in CSV format
- ‚úÖ Top-tier prediction ranking (within reasonable distance of best performer)
- ‚úÖ Ability to defend all modeling decisions during in-person presentation
- ‚úÖ Grade target: 9/10 from presentation + up to 1/10 from prediction ranking

***

## **2. Data Specifications**

### **Dataset Characteristics**
- **Observations:** 20,885 ICU stays (18,702 after cleaning)
- **Features:** 44 total variables ‚Üí ~70+ after feature engineering
- **Target:** `HOSPITAL_EXPIRE_FLAG` (binary: 0=survived, 1=died)
- **Class Distribution:** 88.8% survival, 11.2% mortality

### **Variables Removed**
**ID Columns (kept for tracking, dropped before modeling):**
- `subject_id`, `hadm_id`, `icustay_id`

**Leakage Variables (if present):**
- `DOD` (Date of Death) - directly reveals mortality
- `DEATHTIME` (Death datetime) - directly reveals mortality  
- `LOS` (Length of Stay) - separate regression target

**Data Quality:**
- Dropped 2,183 rows with 21/24 vitals missing (10.5% of data)
- Final training set: 18,702 rows

### **Feature Categories**

**Vital Signs (24 features):** Min/Max/Mean for each
- Heart Rate, Systolic BP, Diastolic BP, Mean BP, Respiration Rate, Temperature (¬∞C), SpO2, Glucose

**Demographics & Clinical (7 categorical):**
- `GENDER`, `ADMISSION_TYPE`, `INSURANCE`, `RELIGION`, `MARITAL_STATUS`, `ETHNICITY`, `FIRST_CAREUNIT`

**Temporal Features:**
- `age_at_admission` (engineered from DOB and ADMITTIME)

**Diagnosis Features (4 engineered from external MIMIC_diagnoses):**
- `diagnosis_count`: Total diagnoses per patient
- `avg_diagnosis_severity`: Mean mortality risk across all diagnoses
- `max_diagnosis_severity`: Highest mortality risk diagnosis
- `primary_diagnosis_severity`: Mortality risk of primary diagnosis (SEQ_NUM=1)

**Text Features (not used in baseline models):**
- `DIAGNOSIS` (free text), `ICD9_diagnosis` (code)

***

## **3. Pipeline Architecture - IMPLEMENTED**

### **‚úÖ Phase 1: Exploratory Data Analysis (COMPLETE)**

**Completed Analyses:**
1. ‚úÖ Data types, missing values, distributions
2. ‚úÖ Class imbalance visualization (88.8% survival)
3. ‚úÖ Vital signs correlation analysis
4. ‚úÖ Age distribution and mortality relationship
5. ‚úÖ Missing data patterns (vital signs missingness)
6. ‚úÖ Categorical variable distributions

**External Diagnoses EDA:**
7. ‚úÖ Distribution of diagnoses per patient (median: ~15, range: 1-39)
8. ‚úÖ Top 20 most common primary diagnoses
9. ‚úÖ Diagnosis count vs mortality relationship (positive correlation)
10. ‚úÖ Diagnosis severity vs mortality (clear gradient by quartile)

**Key Findings:**
- Patients with more diagnoses have higher mortality (complexity proxy)
- Missing vital signs pattern: 2,183 patients had 87.5% vitals missing ‚Üí dropped
- Remaining missingness is sparse and random (suitable for median imputation)

***

### **‚úÖ Phase 2: Data Preprocessing & Feature Engineering (COMPLETE)**

**2.1 External Data Integration: Diagnosis Features** ‚úÖ

**Implementation: Function-based approach**

```python
def fit_diagnosis_features(train_df, diagnoses_df):
    """
    Compute diagnosis-based features using ONLY training data.
    Returns: processed_df, X, y, state_dict
    """
    # 1. Count diagnoses per patient
    # 2. Calculate Bayesian-smoothed severity scores per ICD9 code
    # 3. Aggregate: avg, max, primary diagnosis severity
    # 4. Handle missing diagnoses (fill with overall_mortality)
```

**Severity Scoring Formula:**
$$\text{severity}_{\text{ICD9}} = \frac{n \times \text{raw\_mortality} + \alpha \times \text{overall\_mortality}}{n + \alpha}$$

Where:
- \( n \) = number of patients with this ICD9 code
- \( \alpha = 10 \) = Bayesian smoothing parameter (adds "virtual patients")
- \( \text{overall\_mortality} \) = baseline mortality rate (~0.112)

**Rationale:**
- Rare diagnoses (n < 10) shrink toward overall mortality (prevents overfitting)
- Common diagnoses (n > 100) stay close to observed rates
- No arbitrary thresholds needed

**2.2 Core Preprocessing Pipeline** ‚úÖ

**Implementation: Function-based approach**

```python
def fit_preprocessing_pipeline(df, target_col):
    """
    Fit all preprocessing transformers on training data.
    Returns: processed_df, X, y, state_dict
    """
    # 1. Impute vitals (median) and categoricals (mode)
    # 2. One-hot encode categoricals (drop_first=True)
    # 3. Scale numerical features (StandardScaler)
    # 4. Return fitted transformers in state dict
```

```python
def apply_preprocessing_pipeline(df, state):
    """
    Apply fitted transformers to test data.
    Handles column alignment automatically.
    """
```

**Missing Value Strategy:**
- **Vitals (24 features):** Median imputation (robust to outliers)
- **Categoricals (7 features):** Mode imputation (most common category)
- **Diagnosis features:** Pre-handled (0 count, overall_mortality for severity)

**Categorical Encoding:**
- One-hot encoding with `drop_first=True` (prevents multicollinearity)
- Creates ~30-40 binary features from 7 categoricals

**Feature Scaling:**
- StandardScaler (z-score normalization: mean=0, std=1)
- Applied to continuous features only (excludes binary dummies)
- Fitted on training data, saved for test set

**Critical Design:** All transformers fit on training data only, saved in `state` dict for test set application (prevents leakage)

***

### **‚è≥ Phase 3: Model Development (IN PROGRESS - Tonight)**

**3.1 Train/Validation Strategy**
- Stratified 5-fold cross-validation (maintains 88.8/11.2 class balance)
- Evaluation metric: ROC-AUC (primary), PR-AUC, F1, Recall

**3.2 Baseline Models**
Test 3 algorithm families:
1. **Logistic Regression** (interpretable baseline, fast)
   - `class_weight='balanced'` to handle imbalance
2. **Random Forest** (non-linear, feature importance)
   - `class_weight='balanced'`
   - `n_estimators=100-200`
3. **XGBoost** (gradient boosting, typically best performer)
   - `scale_pos_weight` for class imbalance
   - `n_estimators=100-300`

**3.3 Handling Class Imbalance**
Approaches to test:
- `class_weight='balanced'` (LogReg, RF)
- `scale_pos_weight` (XGBoost)
- Potentially SMOTE if baseline performance insufficient

**3.4 Hyperparameter Tuning**
Use GridSearchCV on best baseline model:

**XGBoost parameters to tune:**
```python
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}
```

**3.5 Model Evaluation**
- Cross-validation ROC-AUC scores
- Feature importance plots (top 20 features)
- Confusion matrix on validation fold
- Calibration plots (predicted probabilities vs actual outcomes)

***

### **‚è≥ Phase 4: Final Model Selection (Tonight)**

**Selection Criteria:**
1. Best 5-fold CV ROC-AUC score
2. Stable performance across folds (low std deviation)
3. Reasonable training time
4. No signs of overfitting

**Optional Ensemble:**
- If time permits: average predictions from top 2 models
- Typically improves robustness

***

### **‚è≥ Phase 5: Test Predictions (Tonight)**

**Critical Implementation:**

```python
# 1. Load test data
test = pd.read_csv('test.csv')

# 2. Apply diagnosis features (using saved diag_state)
test_with_diag = apply_diagnosis_features(test, MIMIC_diagnoses, diag_state)

# 3. Apply preprocessing pipeline (using saved preprocess_state)
X_test = apply_preprocessing_pipeline(test_with_diag, preprocess_state)

# 4. Generate probability predictions
predictions = final_model.predict_proba(X_test)[:, 1]

# 5. Create submission file
submission = pd.DataFrame({
    'id': test['icustay_id'],  # or appropriate ID column
    'prediction': predictions
})
submission.to_csv('argote_mariajose_CML_2025.csv', index=False)
```

**Verification Checklist:**
- [ ] X_test.shape == X_train.shape (same number of features)[1]
- [ ] No NaN values in predictions
- [ ] Predictions are probabilities (range 0-1)
- [ ] Submission file has correct format
- [ ] Filename matches required format

***

## **4. Code Organization - IMPLEMENTED**

### **Notebook Structure**
```
1. ‚úÖ Project Setup & Imports
2. ‚úÖ Data Loading & Overview
3. ‚úÖ Exploratory Data Analysis
   3.1 ‚úÖ Target Distribution
   3.2 ‚úÖ Missing Data Analysis
   3.3 ‚úÖ Vital Signs EDA
   3.4 ‚úÖ Age Analysis
   3.5 ‚úÖ Categorical Features EDA
   3.6 ‚úÖ Feature Correlations
   3.7 ‚úÖ Data Cleaning (drop extreme missingness rows)
   3.8 ‚úÖ External Diagnoses Data EDA
4. ‚úÖ Feature Engineering
   4.1 ‚úÖ Age Calculation
   4.2 ‚úÖ Diagnosis Features (fit_diagnosis_features, apply_diagnosis_features)
   4.3 ‚úÖ Drop Leakage/ID Columns
5. ‚úÖ Data Preprocessing
   5.1 ‚úÖ Missing Value Imputation (fit_preprocessing_pipeline)
   5.2 ‚úÖ Categorical Encoding
   5.3 ‚úÖ Feature Scaling
6. ‚è≥ Model Development (TONIGHT)
   6.1 Baseline Models
   6.2 Cross-Validation Evaluation
   6.3 Hyperparameter Tuning
   6.4 Feature Importance Analysis
7. ‚è≥ Final Model Selection
8. ‚è≥ Test Set Predictions
9. ‚è≥ Conclusions & Reflection
```

### **Key Functions Implemented**
```python
# Diagnosis features
fit_diagnosis_features(train_df, diagnoses_df) ‚Üí df, X, y, state
apply_diagnosis_features(test_df, diagnoses_df, state) ‚Üí df

# Preprocessing pipeline
fit_preprocessing_pipeline(df, target_col) ‚Üí df, X, y, state
apply_preprocessing_pipeline(df, state) ‚Üí X
```

**Benefits:**
- ‚úÖ No code duplication (train/test use same functions)
- ‚úÖ No leakage (all transformers fit on training data only)
- ‚úÖ Perfect alignment (test features automatically match training)
- ‚úÖ Reproducible (state dicts saved with joblib)

***

## **5. Current Progress Status**

### **‚úÖ COMPLETED (70%)**
- [x] Full EDA (base data + diagnoses)
- [x] Data cleaning (drop extreme missingness)
- [x] Age feature engineering
- [x] Diagnosis features (4 new features)
- [x] Missing value imputation (vitals + categoricals)
- [x] Categorical encoding (one-hot)
- [x] Feature scaling (StandardScaler)
- [x] Function-based preprocessing pipeline
- [x] Leakage/ID columns removed
- [x] Final training data: X_train (18,702 √ó ~70+), y_train

### **‚è≥ REMAINING (30% - TONIGHT)**
- [ ] Train 3 baseline models (30 min)
- [ ] Cross-validation evaluation (30 min)
- [ ] Hyperparameter tuning with GridSearchCV (1-2 hours)
- [ ] Feature importance visualization (15 min)
- [ ] Final model selection (15 min)
- [ ] Test set preprocessing (15 min)
- [ ] Generate predictions and CSV (15 min)
- [ ] Final documentation and markdown cells (30 min)
- [ ] Run notebook top-to-bottom verification (15 min)

**Estimated Time Remaining:** 4-5 hours  
**Deadline Buffer:** ~12 hours until submission

***

## **6. Presentation Defense Preparation**

### **Questions to Prepare For**

**1. Diagnosis Severity Score**
- **Q:** "What is severity and how did you calculate it?"
- **A:** "Severity is the mortality rate per ICD9 code, with Bayesian smoothing to handle rare diagnoses. Formula: (n √ó observed_rate + 10 √ó overall_rate) / (n + 10). This balances observed data with prior knowledge, preventing overfitting on rare codes."

**2. Preprocessing Decisions**
- **Q:** "Why median imputation for vitals?"
- **A:** "Median is robust to outliers, which are common in vital signs. Also preserves the distribution shape better than mean for skewed data."

**3. Feature Engineering**
- **Q:** "Why did you drop 2,183 rows?"
- **A:** "They had 87.5% of vital signs missing (21/24 features). Since vitals are core predictors for mortality, these rows would add noise. Mortality rate in dropped rows was similar to overall (0.113 vs 0.112), so no bias introduced."

**4. Model Choice**
- **Q:** "Why did you choose [model X]?"
- **A:** "It achieved the best 5-fold CV ROC-AUC score of [X.XXX] with low variance across folds ([¬±0.XX]). I also tested [other models] but they performed worse."

**5. Handling Class Imbalance**
- **Q:** "How did you handle the 88/12 class split?"
- **A:** "I used class_weight='balanced' for LogReg/RF and scale_pos_weight for XGBoost, which automatically adjusts the loss function to penalize false negatives more heavily."

**6. Feature Count**
- **Q:** "You have ~70 features. Did you consider dimensionality reduction?"
- **A:** "I prioritized interpretability and let tree-based models handle feature selection naturally. Feature importance plots showed [top features]. With 18,702 samples, 70 features is a healthy ratio (>250 samples per feature)."

**7. Test Set Processing**
- **Q:** "How did you ensure no data leakage?"
- **A:** "All transformers (imputers, scalers, diagnosis severity scores) were fit exclusively on training data and saved in state dictionaries. Test data only had `.transform()` applied, never `.fit()` or `.fit_transform()`."

**8. Bayesian Smoothing Parameter**
- **Q:** "Why alpha=10 for diagnosis severity?"
- **A:** "It acts like adding 10 virtual patients to each ICD9 code. This balances trust in observed data (for common codes) with regression to the mean (for rare codes). I could validate this via cross-validation, but 10 is a standard choice in Bayesian shrinkage."

***

## **7. Deliverables Checklist**

### **Before December 11 @ 08:00**
- [ ] Complete Jupyter notebook: `Argote_Mariajose_CML_2025.ipynb`
- [ ] All code cells run top-to-bottom without errors
- [ ] Markdown documentation for every section
- [ ] Code produces submitted predictions
- [ ] Notebook uploaded to submission system

### **Before December 11 @ 18:00**
- [ ] Test predictions file: `argote_mariajose_CML_2025.csv`
- [ ] Predictions are probabilities (from .predict_proba[:, 1])
- [ ] Correct format matching sample submission
- [ ] File uploaded to Kaggle/submission platform

### **Before December 12 @ 10:00**
- [ ] Review notebook thoroughly (every line)
- [ ] Practice explaining diagnosis severity calculation
- [ ] Practice explaining preprocessing pipeline
- [ ] Test running notebook from scratch in clean environment
- [ ] Prepare answers to 8 key questions above

***

## **8. Risk Mitigation**

### **Common Pitfalls - ACTIVELY AVOIDED**
1. ‚úÖ **Data leakage:** All transformers fit on training data only (state dict approach)
2. ‚úÖ **Test set contamination:** Separate `fit_*` and `apply_*` functions enforce this
3. ‚è≥ **Overfitting:** Using 5-fold CV, will avoid excessive hyperparameter tuning
4. ‚è≥ **Wrong prediction format:** Will verify probabilities (0-1 range), not classes
5. ‚è≥ **Code doesn't run:** Will test full execution before submission
6. ‚úÖ **Missing justifications:** Markdown cells document every decision
7. ‚úÖ **Column misalignment:** `apply_preprocessing_pipeline()` handles automatically

***

## **9. Success Metrics**

**Current Status:** On track for Target Product

**Minimum Viable Product (7-8/10):**
- ‚úÖ Notebook runs end-to-end
- ‚è≥ Predictions submitted on time
- ‚è≥ Can explain all code and decisions

**Target Product (9-10/10):**
- ‚úÖ Clean, well-documented pipeline
- ‚è≥ Multiple models evaluated with cross-validation
- ‚è≥ GridSearch hyperparameter tuning
- ‚è≥ Top 25% of prediction ranking
- ‚è≥ Confident presentation defense

**Stretch Goals (Full 10/10):**
- ‚è≥ Ensemble methods (if time)
- ‚úÖ Advanced feature engineering (diagnosis severity)
- ‚è≥ Top 10% prediction ranking
- ‚úÖ Function-based architecture (reusable, clean)

***

## **10. Tonight's Action Plan (5:30 PM - 10:30 PM)**

### **Block 1: Baseline Models (6:00 PM - 6:30 PM)**
- Train Logistic Regression, Random Forest, XGBoost
- 5-fold stratified CV with ROC-AUC scoring
- Document initial scores

### **Block 2: Hyperparameter Tuning (6:30 PM - 8:30 PM)**
- GridSearchCV on best baseline model
- Test 3-5 parameter combinations per hyperparameter
- Select best model based on CV score

### **Block 3: Test Predictions (8:30 PM - 9:15 PM)**
- Apply preprocessing to test set
- Generate probability predictions
- Create submission CSV
- Verify format and values

### **Block 4: Documentation & Verification (9:15 PM - 10:00 PM)**
- Add markdown cells explaining model results
- Feature importance visualization
- Run notebook top-to-bottom
- Fix any errors

### **Block 5: Buffer (10:00 PM - 10:30 PM)**
- Final review
- Practice presentation answers
- Early submission if ready

***

## **11. File Artifacts to Save**

**Preprocessing State:**
- `preprocessing_state.pkl` (imputers, scaler, column lists)
- `diagnosis_state.pkl` (severity_map, overall_mort, alpha)

**Models (optional for debugging):**
- `final_model.pkl` (best trained model)

**Outputs:**
- `argote_mariajose_CML_2025.csv` (submission file)
- `Argote_Mariajose_CML_2025.ipynb` (final notebook)

***

## **END OF PRD**

**Last Commit:** Dec 10, 2025, 5:23 PM CET  
**Next Milestone:** Model training (tonight)  
**Confidence Level:** HIGH ‚úÖ

***

**Copy this entire markdown and save it as `mimic_prd_UPDATED.md` in your project folder.** You're 70% done and on track for a 9-10/10 grade. Keep pushing! üí™